LLM_API="local"
# Specify argument GPU device_id like "0" "0,1"
DEVICE_ID="3"
GPUID1=3
GPUID2=3
# <runtime_backend>: Specify argument LLM inference runtime backend, options={default, hf, vllm}
RUNTIME_BACKEND="default"
# Specify argument the path to load LLM model using FastChat serve API, options={Qwen-7B-Chat, deepseek-llm-7b-chat, ...}
MODEL_NAME=""
# Specify argument the conversation template according to the LLM model when using FastChat serve API, options={qwen-7b-chat, deepseek-chat, ...}
CONV_TEMPLATE=""
# Use options {1, 2} to set tensor parallel parameters for vllm backend when using FastChat serve API, default tensor_parallel=1
TP=1
# Specify argument gpu_memory_utilization (0,1] for vllm backend when using FastChat serve API, default gpu_memory_utilization=0.81
GPU_MEM_UTILI=0.81

RERANK_PORT=10001
EMBED_PORT=10001
LLM_API_SERVE_PORT=20000
LLM_API_SERVE_MODEL=Qwen1.5-7B-Chat
RERANK_PORT=10001
EMBED_PORT=10001
LLM_API_SERVE_PORT=20000
LLM_API_SERVE_MODEL=Qwen1.5-7B-Chat
RERANK_PORT=10001
EMBED_PORT=10001
LLM_API_SERVE_PORT=20000
LLM_API_SERVE_MODEL=Qwen1.5-7B-Chat
RERANK_PORT=10001
EMBED_PORT=10001
